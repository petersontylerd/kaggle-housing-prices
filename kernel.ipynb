{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [EDA](#EDA)\n",
    "    1. [Category feature EDA](#Category-feature-EDA)\n",
    "    1. [Count feature EDA](#Count-feature-EDA)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Missing data](#Missing-data)\n",
    "    1. [Feature engineering](#Feature-engineering)\n",
    "        1. [Handcrafted](#Handcrafted)\n",
    "        1. [Polynomial features](#Polynomial-features)\n",
    "        1. [Encoding](#Encoding)\n",
    "    1. [Feature transformation](#Feature-transformation)\n",
    "        1. [Skew correction](#Skew-correction)\n",
    "        1. [Scaling](#Scaling)     \n",
    "    1. [Outliers](#Outliers)\n",
    "    1. [Additional exploratory data analysis](#Additional-exploratory-data-analysis)\n",
    "1. [Feature importance](#Feature-importance)    \n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation-1)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "    1. [Model performance evaluation - standard models](#Model-performance-evaluation-standard-models)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "    1. [Submission - standard models](#Submission-standard-models)\n",
    "1. [Stacking](#Stacking)\n",
    "    1. [Primary models](#Primary-models)\n",
    "    1. [Meta model](#Meta-model)                \n",
    "    1. [Model performance evaluation - stacked models](#Model-performance-evaluation-stacked-models)\n",
    "    1. [Submission - stacked models](#Submission-stacked-models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:15:15.202249Z",
     "start_time": "2020-03-24T15:15:13.250946Z"
    },
    "code_folding": [
     32,
     44,
     52,
     62,
     75,
     113
    ]
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    IsolationForest,\n",
    ")\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import (\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    LinearRegression,\n",
    "    SGDRegressor,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    PolynomialFeatures,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    KBinsDiscretizer,\n",
    "    QuantileTransformer,\n",
    "    PowerTransformer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from category_encoders import (\n",
    "    WOEEncoder,\n",
    "    TargetEncoder,\n",
    "    CatBoostEncoder,\n",
    "    BinaryEncoder,\n",
    "    CountEncoder,\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import eif\n",
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "import squarify\n",
    "\n",
    "sys.path.append(f\"{os.environ['REPOS']}/mlmachine\")\n",
    "sys.path.append(f\"{os.environ['REPOS']}/prettierplot\")\n",
    "\n",
    "import mlmachine as mlm\n",
    "import mlmachine.data as data\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    KFoldEncoder,\n",
    "    GroupbyImputer,\n",
    "    PandasFeatureUnion,\n",
    "    DualTransformer,\n",
    ")\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "\n",
    "# experiment_path_root = \"/data/t1-tpeterso/repos/kaggle-housing-prices/experiments/housing_prices\"\n",
    "# experiment = \"\"\n",
    "\n",
    "# # reload objects\n",
    "# machine = pickle.load(open(os.path.join(experiment_path_root, experiment, \"machine\", \"machine.pkl\"), 'rb'))\n",
    "# impute_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"impute_pipe.pkl\"), 'rb'))\n",
    "# polynomial_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"polynomial_pipe.pkl\"), 'rb'))\n",
    "# encode_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"encode_pipe.pkl\"), 'rb'))\n",
    "# target_encode_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"target_encode_pipe.pkl\"), 'rb'))\n",
    "# skew_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"skew_pipe.pkl\"), 'rb'))\n",
    "# scale_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"scale_pipe.pkl\"), 'rb'))\n",
    "# fs = pickle.load(open(os.path.join(experiment_path_root, experiment, \"feature_selection\", \"FeatureSelector.pkl\"), 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:19.687846Z",
     "start_time": "2020-02-02T04:21:19.518294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "df_train, df_valid = data.housing()\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(df_train.shape))\n",
    "print(\"Validation data dimensions: {}\".format(df_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:19.887565Z",
     "start_time": "2020-02-02T04:21:19.781339Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "df_train.info()\n",
    "display(df_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:19.912494Z",
     "start_time": "2020-02-02T04:21:19.895065Z"
    }
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "df_train.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create machine object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:19.932091Z",
     "start_time": "2020-02-02T04:21:19.914973Z"
    },
    "code_folding": [
     0,
     21,
     26,
     38,
     67
    ]
   },
   "outputs": [],
   "source": [
    "continuous = [\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "]\n",
    "\n",
    "remove_features = [\n",
    "    \"Id\",\n",
    "    \"MiscVal\",\n",
    "]\n",
    "\n",
    "count = [\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageCars\",\n",
    "]\n",
    "\n",
    "nominal = [\n",
    "    \"MSSubClass\",\n",
    "    \"MSZoning\",    \n",
    "    \"LandContour\",\n",
    "    \"Neighborhood\",\n",
    "    \"Condition1\",\n",
    "    \"Condition2\",\n",
    "    \"BldgType\",\n",
    "    \"HouseStyle\",\n",
    "#     \"YearBuilt\",\n",
    "#     \"YearRemodAdd\",\n",
    "    \"RoofStyle\",\n",
    "    \"RoofMatl\",\n",
    "    \"Exterior1st\",\n",
    "    \"Exterior2nd\",\n",
    "    \"MasVnrType\",\n",
    "    \"Foundation\",\n",
    "    \"Heating\",\n",
    "    \"GarageType\",\n",
    "#     \"GarageYrBlt\", \n",
    "    \"Fence\",\n",
    "    \"SaleType\",\n",
    "    \"SaleCondition\",\n",
    "#     \"MiscFeature\",    \n",
    "#     \"MoSold\",\n",
    "#     \"YrSold\",\n",
    "    \n",
    "]\n",
    "\n",
    "ordinal = [\n",
    "    \"Street\",  \n",
    "    \"Alley\" ,\n",
    "    \"LotShape\", \n",
    "    \"Utilities\", \n",
    "    \"LotConfig\",\n",
    "    \"LandSlope\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",    \n",
    "    \"ExterQual\", \n",
    "    \"ExterCond\", \n",
    "    \"BsmtQual\", \n",
    "    \"BsmtCond\", \n",
    "    \"BsmtExposure\", \n",
    "    \"BsmtFinType1\", \n",
    "    \"BsmtFinType2\", \n",
    "    \"HeatingQC\", \n",
    "    \"CentralAir\", \n",
    "    \"Electrical\", \n",
    "    \"KitchenQual\", \n",
    "    \"Functional\", \n",
    "    \"FireplaceQu\", \n",
    "    \"GarageFinish\", \n",
    "    \"GarageQual\", \n",
    "    \"GarageCond\", \n",
    "    \"PavedDrive\", \n",
    "    \"PoolQC\",    \n",
    "]\n",
    "\n",
    "ordinal_encodings = {\n",
    "    \"Street\": [\"Grvl\", \"Pave\"],\n",
    "    \"Alley\": [\"Nonexistent\", \"Grvl\", \"Pave\"],\n",
    "    \"LotShape\": [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],\n",
    "    \"Utilities\": [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"LotConfig\": [\"FR3\", \"FR2\", \"Corner\", \"Inside\", \"CulDSac\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"OverallQual\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"OverallCond\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"ExterQual\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"], \n",
    "    \"ExterCond\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtQual\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtCond\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtExposure\": [\"Nonexistent\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFinType1\": [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],\n",
    "    \"HeatingQC\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"FuseP\", \"FuseF\", \"FuseA\", \"Mix\", \"SBrkr\"],\n",
    "    \"KitchenQual\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"FireplaceQu\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"GarageFinish\": [\"Nonexistent\", \"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"GarageQual\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"GarageCond\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"PoolQC\": [\"Nonexistent\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df_train, df_valid = mlm.train_test_df_compile(data=df_train, target_col=\"SalePrice\")\n",
    "\n",
    "machine = mlm.Machine(\n",
    "    experiment_name=\"housing_price_regression\",\n",
    "    training_dataset=df_train,\n",
    "    validation_dataset=df_valid,    \n",
    "    target=\"SalePrice\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review mlm dtypes\n",
    "machine.training_features.mlm_dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:21.555621Z",
     "start_time": "2020-02-02T04:21:20.197001Z"
    }
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in machine.training_features.mlm_dtypes[\"category\"]:\n",
    "    machine.eda_num_target_cat_feat(\n",
    "        feature=feature,\n",
    "        level_count_cap=10,\n",
    "        chart_scale=20,\n",
    "        training_data=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:22.999577Z",
     "start_time": "2020-02-02T04:21:21.564569Z"
    }
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in machine.training_features.mlm_dtypes[\"count\"]:\n",
    "    machine.eda_num_target_cat_feat(\n",
    "        feature=feature,\n",
    "#         level_count_cap=20,\n",
    "        chart_scale=20,\n",
    "        training_data=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:27.664677Z",
     "start_time": "2020-02-02T04:21:23.002675Z"
    }
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "for feature in machine.training_features.mlm_dtypes[\"continuous\"]:\n",
    "    machine.eda_num_target_num_feat(\n",
    "        feature=feature,\n",
    "#         outliers_out_of_scope=5,\n",
    "        chart_scale=20,\n",
    "        training_data=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:29.072874Z",
     "start_time": "2020-02-02T04:21:27.675110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot(chart_scale=25)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(df=machine.training_features, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:21:29.814889Z",
     "start_time": "2020-02-02T04:21:29.076395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_scale=15)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap_target(df=machine.training_features, target=machine.training_target, thresh=0.6, annot = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:31.079312Z",
     "start_time": "2020-02-02T04:21:29.819900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_scale=10)\n",
    "p.pair_plot(\n",
    "    df=machine.training_features,\n",
    "    columns=[\n",
    "        \"LotFrontage\",\n",
    "        \"LotArea\",\n",
    "        \"MasVnrArea\",\n",
    "        \"BsmtFinSF1\",\n",
    "        \"BsmtFinSF2\",\n",
    "        \"BsmtUnfSF\",\n",
    "#         \"TotalBsmtSF\",\n",
    "#         \"1stFlrSF\",\n",
    "#         \"2ndFlrSF\",\n",
    "#         \"GrLivArea\",\n",
    "#         \"TotRmsAbvGrd\",\n",
    "#         \"GarageYrBlt\",\n",
    "#         \"GarageArea\",\n",
    "#         \"WoodDeckSF\",\n",
    "#         \"OpenPorchSF\",\n",
    "    ],\n",
    "    diag_kind=\"kde\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:31.786975Z",
     "start_time": "2020-02-02T04:22:31.081244Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate distribution of target variable\n",
    "machine.eda_transform_target(data=machine.training_target, name=machine.training_target.name)\n",
    "machine.eda_transform_log1(data=machine.training_target, name=machine.training_target.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:31.795226Z",
     "start_time": "2020-02-02T04:22:31.791011Z"
    }
   },
   "outputs": [],
   "source": [
    "# log + 1 transform target\n",
    "machine.training_target = np.log1p(machine.training_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:32.466626Z",
     "start_time": "2020-02-02T04:22:31.799060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "machine.eda_missing_summary(chart_scale=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:33.260767Z",
     "start_time": "2020-02-02T04:22:32.474459Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(machine.training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:34.017781Z",
     "start_time": "2020-02-02T04:22:33.263036Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(machine.training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:35.752760Z",
     "start_time": "2020-02-02T04:22:34.020090Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(machine.training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:38.286485Z",
     "start_time": "2020-02-02T04:22:35.758033Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(machine.training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:39.053595Z",
     "start_time": "2020-02-02T04:22:38.292803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "machine.eda_missing_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:39.775217Z",
     "start_time": "2020-02-02T04:22:39.057081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(machine.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:40.528434Z",
     "start_time": "2020-02-02T04:22:39.781255Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(machine.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:43.032079Z",
     "start_time": "2020-02-02T04:22:40.532292Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(machine.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:45.768989Z",
     "start_time": "2020-02-02T04:22:43.035239Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(machine.validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs. validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:45.788304Z",
     "start_time": "2020-02-02T04:22:45.771835Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "machine.missing_column_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:45.961876Z",
     "start_time": "2020-02-02T04:22:45.790434Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute pipeline\n",
    "category_constant = ['GarageFinish', 'Alley', 'MasVnrType', 'GarageType', 'BsmtFinType1',\n",
    "                       'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'PoolQC', 'GarageCond',\n",
    "                       'FireplaceQu', 'GarageQual', 'Fence', 'BsmtExposure', 'MiscFeature']\n",
    "number_constant = [\"GarageYrBlt\",\"MasVnrArea\",\"BsmtUnfSF\",\"GarageArea\",\"BsmtFinSF1\",\"TotalBsmtSF\",\"BsmtFinSF2\"]\n",
    "category_mode = [\"Electrical\",\"Functional\",\"SaleType\",\"Exterior1st\",\"MSZoning\",\"Exterior2nd\",\"KitchenQual\",\"Utilities\"]\n",
    "number_mode = [\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"]\n",
    "\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"catConstant\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=category_constant),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"Nonexistent\"))\n",
    "    )),\n",
    "    (\"numConstant\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=number_constant),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "    )),\n",
    "    (\"catMode\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=category_mode),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"numMode\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=number_mode),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"LotFrontage\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"LotFrontage\",\"Neighborhood\"]),\n",
    "        GroupbyImputer(null_column=\"LotFrontage\", groupby_column=\"Neighborhood\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"LotFrontage\"] + category_constant + number_constant + category_mode + number_mode),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "impute_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"impute_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(impute_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = impute_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = impute_pipe.transform(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:45.984945Z",
     "start_time": "2020-02-02T04:22:45.963895Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "machine.eda_missing_summary(training_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:46.007119Z",
     "start_time": "2020-02-02T04:22:45.990423Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "machine.eda_missing_summary(training_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:46.115769Z",
     "start_time": "2020-02-02T04:22:46.012001Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "machine.training_features[\"BsmtFinSF\"] = machine.training_features[\"BsmtFinSF1\"] + machine.training_features[\"BsmtFinSF2\"]\n",
    "machine.training_features[\"TotalSF\"] = (\n",
    "    machine.training_features[\"TotalBsmtSF\"] + machine.training_features[\"1stFlrSF\"] + machine.training_features[\"2ndFlrSF\"]\n",
    ")\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:49.739820Z",
     "start_time": "2020-02-02T04:22:46.118223Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate additional features\n",
    "for feature in [\"BsmtFinSF\",\"TotalSF\"]:\n",
    "    machine.eda_num_target_num_feat(\n",
    "        feature=feature,\n",
    "#         outliers_out_of_scope=5,\n",
    "        chart_scale=20\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:49.782227Z",
     "start_time": "2020-02-02T04:22:49.744646Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "machine.validation_features[\"BsmtFinSF\"] = machine.validation_features[\"BsmtFinSF1\"] + machine.validation_features[\"BsmtFinSF2\"]\n",
    "machine.validation_features[\"TotalSF\"] = (\n",
    "    machine.validation_features[\"TotalBsmtSF\"] + machine.validation_features[\"1stFlrSF\"] + machine.validation_features[\"2ndFlrSF\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:49.908454Z",
     "start_time": "2020-02-02T04:22:49.784485Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform pipe\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"TotalSF\",\"BsmtFinSF\"]),\n",
    "        #DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasTransformer(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"TotalSF\",\"BsmtFinSF\"]),\n",
    "        #DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "polynomial_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"polynomial_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(polynomial_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = polynomial_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = polynomial_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:49.942075Z",
     "start_time": "2020-02-02T04:22:49.910324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data object columns\n",
    "machine.training_features[machine.training_features.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:50.011362Z",
     "start_time": "2020-02-02T04:22:49.944469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "machine.unique_category_levels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:50.123387Z",
     "start_time": "2020-02-02T04:22:50.035572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "machine.validation_features[machine.validation_features.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:50.178553Z",
     "start_time": "2020-02-02T04:22:50.125632Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each object columns\n",
    "machine.unique_category_levels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:50.257018Z",
     "start_time": "2020-02-02T04:22:50.181038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "machine.compare_train_valid_levels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:55.642570Z",
     "start_time": "2020-02-02T04:22:50.260093Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# encode pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=nominal),\n",
    "        PandasTransformer(OneHotEncoder(drop=None, handle_unknown=\"ignore\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "#     (\"bin\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"continuous\"]),\n",
    "#         PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "#     )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=nominal + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "encode_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = encode_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:17.050511Z",
     "start_time": "2020-02-02T04:22:55.645687Z"
    }
   },
   "outputs": [],
   "source": [
    "# target encoding pipe\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=machine.training_target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=machine.training_target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "target_encode_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"target_encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(target_encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = target_encode_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = target_encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:42.211612Z",
     "start_time": "2020-02-02T04:25:39.117274Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of number features - training data\n",
    "machine.skew_summary(training_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:39.114120Z",
     "start_time": "2020-02-02T04:25:17.053965Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of number features - validation data\n",
    "machine.skew_summary(training_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:42.218790Z",
     "start_time": "2020-02-02T04:25:42.214685Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correction pipeline\n",
    "skew_pipe = PandasFeatureUnion([\n",
    "    (\"skew\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# # fit & save objects\n",
    "# skew_pipe.fit(machine.training_features)\n",
    "# with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"skew_pipe.pkl\"), 'wb') as handle:\n",
    "#     pickle.dump(skew_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # transform datasets\n",
    "# machine.training_features = skew_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = skew_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:44.309617Z",
     "start_time": "2020-02-02T04:25:42.222341Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", make_pipeline(\n",
    "        DataFrameSelector(),\n",
    "        PandasTransformer(RobustScaler())\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "scale_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"scale_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(scale_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = scale_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = scale_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:45.973546Z",
     "start_time": "2020-02-02T04:25:44.316821Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",machine.OutlierIQR(\n",
    "                outlier_count=20,\n",
    "                iqr_step=1.5,\n",
    "                features=machine.training_features.mlm_dtypes[\"number\"],\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "machine.training_features = train_pipe.transform(machine.training_features)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(iqr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:47.666084Z",
     "start_time": "2020-02-02T04:25:45.994584Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = IsolationForest(\n",
    "#     behaviour=\"new\",\n",
    "    max_samples=machine.training_features.shape[0],\n",
    "    random_state=0,\n",
    "    contamination=0.01\n",
    ")\n",
    "clf.fit(machine.training_features[machine.training_features.columns])\n",
    "preds = clf.predict(machine.training_features[machine.training_features.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(machine.training_features[mask].index)\n",
    "print(if_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:52.444624Z",
     "start_time": "2020-02-02T04:25:47.685003Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",machine.ExtendedIsoForest(\n",
    "                columns=machine.training_features.mlm_dtypes[\"number\"],\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                extension_level=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "machine.training_features = train_pipe.transform(machine.training_features)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(eif_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:52.455639Z",
     "start_time": "2020-02-02T04:25:52.448598Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "# outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:52.482872Z",
     "start_time": "2020-02-02T04:25:52.461354Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = machine.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary[outlier_summary[\"count\"] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:25:52.504414Z",
     "start_time": "2020-02-02T04:25:52.487564Z"
    }
   },
   "outputs": [],
   "source": [
    "# capture index values of known outliers\n",
    "knownOutliers = (\n",
    "    machine.training_features[machine.training_features[\"LotArea\"] > 60000].index.values.tolist()\n",
    "    + machine.training_features[machine.training_features[\"LotFrontage\"] > 300].index.values.tolist()\n",
    "    + machine.training_features[machine.training_features[\"GrLivArea\"] > 4000].index.values.tolist()\n",
    ")\n",
    "knownOutliers = sorted(set(knownOutliers))\n",
    "print(knownOutliers)\n",
    "\n",
    "# index of known outliers and outliers identified with the known outliers removed\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "# print(outliers)\n",
    "\n",
    "# remove outlers from predictors and response\n",
    "# machine.training_features = machine.training_features.drop(outliers)\n",
    "# machine.training_target = machine.training_target.drop(index=outliers)\n",
    "\n",
    "print(machine.training_features.shape)\n",
    "print(machine.training_target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in machine.training_features:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save machine object\n",
    "with open(os.path.join(machine.current_experiment_dir, \"machine\", \"machine.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(machine, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:33:08.080359Z",
     "start_time": "2020-02-02T04:28:02.002204Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "estimators = [\n",
    "    LinearRegression,\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    KernelRidge,\n",
    "    SVR,\n",
    "    LGBMRegressor,\n",
    "    XGBRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    KNeighborsRegressor,\n",
    "]\n",
    "\n",
    "fs = machine.FeatureSelector(\n",
    "    training_features=machine.training_features,\n",
    "    training_target=machine.training_target,\n",
    "    validation_features=machine.validation_features,\n",
    "    validation_target=machine.validation_target,\n",
    "    estimators=estimators,\n",
    "    experiment_dir=machine.current_experiment_dir,\n",
    "    classification=False,\n",
    ")\n",
    "fs.feature_selector_suite(\n",
    "    sequential_scoring=[\"root_mean_squared_error\"],\n",
    "    n_jobs=4,\n",
    "    save_to_csv=True,\n",
    "    verbose=True,\n",
    "    run_sfs=False,\n",
    "    run_sbs=False,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "fs.run_cross_val(\n",
    "    estimators=estimators,\n",
    "    scoring=[\"root_mean_squared_error\"],\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=2,\n",
    "    save_to_csv=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.plot_results(\n",
    "    scoring=\"root_mean_squared_error\",\n",
    "    title_scale=0.8,\n",
    "    show_features=False,\n",
    "    marker_on=False,\n",
    "    save_plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fs.create_cross_val_features_df(scoring=\"root_mean_squared_error\")\n",
    "# fs.cross_val_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fs.create_cross_val_features_dict(scoring=\"root_mean_squared_error\")\n",
    "# fs.cross_val_features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature selector\n",
    "with open(os.path.join(machine.current_experiment_dir, \"feature_selection\", \"FeatureSelector.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(fs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:15:21.108169Z",
     "start_time": "2020-03-24T15:15:15.342267Z"
    },
    "code_folding": [
     4,
     25,
     30,
     42,
     71,
     100,
     183,
     231,
     285,
     300
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# import training data\n",
    "df_train, df_valid = data.housing()\n",
    "\n",
    "continuous = [\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "]\n",
    "\n",
    "remove_features = [\n",
    "    \"Id\",\n",
    "    \"MiscVal\",\n",
    "]\n",
    "\n",
    "count = [\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageCars\",\n",
    "]\n",
    "\n",
    "nominal = [\n",
    "    \"MSSubClass\",\n",
    "    \"MSZoning\",    \n",
    "    \"LandContour\",\n",
    "    \"Neighborhood\",\n",
    "    \"Condition1\",\n",
    "    \"Condition2\",\n",
    "    \"BldgType\",\n",
    "    \"HouseStyle\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"RoofStyle\",\n",
    "    \"RoofMatl\",\n",
    "    \"Exterior1st\",\n",
    "    \"Exterior2nd\",\n",
    "    \"MasVnrType\",\n",
    "    \"Foundation\",\n",
    "    \"Heating\",\n",
    "    \"GarageType\",\n",
    "    \"GarageYrBlt\", \n",
    "    \"Fence\",\n",
    "    \"SaleType\",\n",
    "    \"SaleCondition\",\n",
    "    \"MiscFeature\",    \n",
    "    \"MoSold\",\n",
    "    \"YrSold\",\n",
    "    \n",
    "]\n",
    "\n",
    "ordinal = [\n",
    "    \"Street\",  \n",
    "    \"Alley\" ,\n",
    "    \"LotShape\", \n",
    "    \"Utilities\", \n",
    "    \"LotConfig\",\n",
    "    \"LandSlope\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",    \n",
    "    \"ExterQual\", \n",
    "    \"ExterCond\", \n",
    "    \"BsmtQual\", \n",
    "    \"BsmtCond\", \n",
    "    \"BsmtExposure\", \n",
    "    \"BsmtFinType1\", \n",
    "    \"BsmtFinType2\", \n",
    "    \"HeatingQC\", \n",
    "    \"CentralAir\", \n",
    "    \"Electrical\", \n",
    "    \"KitchenQual\", \n",
    "    \"Functional\", \n",
    "    \"FireplaceQu\", \n",
    "    \"GarageFinish\", \n",
    "    \"GarageQual\", \n",
    "    \"GarageCond\", \n",
    "    \"PavedDrive\", \n",
    "    \"PoolQC\",    \n",
    "]\n",
    "\n",
    "ordinal_encodings = {\n",
    "    \"Street\": [\"Grvl\", \"Pave\"],\n",
    "    \"Alley\": [\"Nonexistent\", \"Grvl\", \"Pave\"],\n",
    "    \"LotShape\": [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],\n",
    "    \"Utilities\": [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"LotConfig\": [\"FR3\", \"FR2\", \"Corner\", \"Inside\", \"CulDSac\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"OverallQual\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"OverallCond\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"ExterQual\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"], \n",
    "    \"ExterCond\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtQual\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtCond\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtExposure\": [\"Nonexistent\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFinType1\": [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"Nonexistent\", \"Unf\", \"LwQ\", \"BLQ\", \"Rec\", \"ALQ\", \"GLQ\"],\n",
    "    \"HeatingQC\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"FuseP\", \"FuseF\", \"FuseA\", \"Mix\", \"SBrkr\"],\n",
    "    \"KitchenQual\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"FireplaceQu\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"GarageFinish\": [\"Nonexistent\", \"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"GarageQual\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"GarageCond\": [\"Nonexistent\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"PoolQC\": [\"Nonexistent\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "}\n",
    "\n",
    "train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"SalePrice\",\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=False,\n",
    ")\n",
    "\n",
    "# additional features\n",
    "machine.training_features[\"BsmtFinSF\"] = machine.training_features[\"BsmtFinSF1\"] + machine.training_features[\"BsmtFinSF2\"]\n",
    "machine.training_features[\"TotalSF\"] = (\n",
    "    machine.training_features[\"TotalBsmtSF\"] + machine.training_features[\"1stFlrSF\"] + machine.training_features[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "#################################################################################\n",
    "# import validation data\n",
    "valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=remove_features,\n",
    "    identify_as_continuous=continuous,\n",
    "    identify_as_count=count,    \n",
    "    identify_as_nominal=nominal,\n",
    "    identify_as_ordinal=ordinal,\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=False,\n",
    ")\n",
    "\n",
    "# change clearly erroneous value to what it probably was\n",
    "machine.validation_features[\"GarageYrBlt\"].replace({2207: 2007}, inplace=True)\n",
    "\n",
    "# additional features\n",
    "machine.validation_features[\"BsmtFinSF\"] = machine.validation_features[\"BsmtFinSF1\"] + machine.validation_features[\"BsmtFinSF2\"]\n",
    "machine.validation_features[\"TotalSF\"] = (\n",
    "    machine.validation_features[\"TotalBsmtSF\"] + machine.validation_features[\"1stFlrSF\"] + machine.validation_features[\"2ndFlrSF\"]\n",
    ")\n",
    "machine.validation_features.loc[machine.validation_features[\"TotalSF\"].isnull(), \"TotalSF\"] = (\n",
    "    machine.validation_features[\"1stFlrSF\"] + machine.validation_features[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "machine.update_dtypes()\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# impute pipeline\n",
    "object_constant = ['GarageFinish', 'Alley', 'MasVnrType', 'GarageType', 'BsmtFinType1',\n",
    "                       'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'PoolQC', 'GarageCond',\n",
    "                       'FireplaceQu', 'GarageQual', 'Fence', 'BsmtExposure', 'MiscFeature']\n",
    "number_constant = [\"GarageYrBlt\",\"MasVnrArea\",\"BsmtUnfSF\",\"GarageArea\",\"BsmtFinSF\",\"BsmtFinSF1\",\"TotalBsmtSF\",\"BsmtFinSF2\"]\n",
    "object_mode = [\"Electrical\",\"Functional\",\"SaleType\",\"Exterior1st\",\"MSZoning\",\"Exterior2nd\",\"KitchenQual\",\"Utilities\"]\n",
    "number_mode = [\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"]\n",
    "\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"catConstant\", make_pipeline(\n",
    "        DataFrameSelector(object_constant),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"Nonexistent\"))\n",
    "    )),\n",
    "    (\"numConstant\", make_pipeline(\n",
    "        DataFrameSelector(number_constant),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "    )),\n",
    "    (\"catMode\", make_pipeline(\n",
    "        DataFrameSelector(object_mode),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"numMode\", make_pipeline(\n",
    "        DataFrameSelector(number_mode),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"LotFrontage\", make_pipeline(\n",
    "        DataFrameSelector([\"LotFrontage\",\"Neighborhood\"]),\n",
    "        GroupbyImputer(null_column=\"LotFrontage\", groupby_column=\"Neighborhood\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns = [\"LotFrontage\"] + object_constant + number_constant + object_mode + number_mode),\n",
    "    )),\n",
    "])\n",
    "\n",
    "machine.training_features = impute_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = impute_pipe.transform(machine.validation_features)\n",
    "\n",
    "# #################################################################################\n",
    "# # polynomial feature pipe\n",
    "# polynomial_pipe = PandasFeatureUnion([\n",
    "#     (\"polynomial\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "#         PandasTransformer(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "#     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"], exclude_columns=[\"Name\",\"Cabin\"]),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = polynomial_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = polynomial_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "# \n",
    "\n",
    "# feature transformation pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=nominal, exclude_columns=[\"YearBuilt\",\"GarageYrBlt\"]),\n",
    "        PandasTransformer(OneHotEncoder(drop=None, handle_unknown=\"ignore\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"continuous\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=nominal + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "machine.training_features = encode_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n",
    "\n",
    "\n",
    "# # target encoding pipe\n",
    "# target_encode_pipe = PandasFeatureUnion([\n",
    "#     (\"target\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "#         KFoldEncoder(\n",
    "#             target=machine.training_target,\n",
    "#             cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "#             encoder=TargetEncoder,\n",
    "#         ),\n",
    "#     )),\n",
    "#     (\"catboost\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "#         KFoldEncoder(\n",
    "#             target=machine.training_target,\n",
    "#             cv=KFold(n_splits=5, shuffle=False, random_state=0),\n",
    "#             encoder=CatBoostEncoder,\n",
    "#         ),\n",
    "#     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = target_encode_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = target_encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "# \n",
    "\n",
    "### scale features\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", make_pipeline(\n",
    "        DataFrameSelector(),\n",
    "        PandasTransformer(RobustScaler())\n",
    "    )),\n",
    "])\n",
    "\n",
    "machine.training_features = scale_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = scale_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# remove outliers\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "machine.training_features = machine.training_features.drop(outliers)\n",
    "machine.training_target = machine.training_target.drop(index=outliers)\n",
    "\n",
    "# log transform target\n",
    "machine.training_target = np.log1p(machine.training_target)\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:15:26.963800Z",
     "start_time": "2020-03-24T15:15:26.822772Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "estimator_parameter_space = {\n",
    "    \"Lasso\": {\"alpha\": hp.uniform(\"alpha\", 0.0000001, 20)},\n",
    "    \"Ridge\": {\"alpha\": hp.uniform(\"alpha\", 0.0000001, 20)},\n",
    "    \"ElasticNet\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.0000001, 20),\n",
    "        \"l1_ratio\": hp.uniform(\"l1_ratio\", 0.0, 0.2),\n",
    "    },\n",
    "    \"KernelRidge\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.000001, 15),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"polynomial\", \"rbf\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "    },\n",
    "    \"LGBMRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "    \"XGBRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    },\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00001, 10),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0001, 10),\n",
    "        \"epsilon\": hp.uniform(\"epsilon\", 0.001, 5),\n",
    "    },\n",
    "    \"KNeighborsRegressor\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "        \"p\": hp.choice(\"p\", [1, 2]),\n",
    "    },\n",
    "}\n",
    "\n",
    "bayes_optim_summary = pd.read_csv(\"bayes_optimization_summary_root_mean_squared_error_2002060358.csv\", na_values=\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:08:06.345694Z",
     "start_time": "2019-07-20T20:08:02.750572Z"
    }
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "machine.exec_bayes_optim_search(\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    training_features=machine.training_features,\n",
    "    training_target=machine.training_target,\n",
    "    validation_features=machine.validation_features,\n",
    "    validation_target=machine.validation_target,\n",
    "    scoring=\"root_mean_squared_error\",\n",
    "    n_folds=5,\n",
    "    n_jobs=2,\n",
    "    iters=250,\n",
    "    show_progressbar=True,\n",
    "    columns=fs.cross_val_features_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loss by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:13:32.856001Z",
     "start_time": "2019-07-20T20:13:24.355066Z"
    }
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(machine.bayes_optim_summary[\"estimator\"]):\n",
    "    machine.model_loss_plot(\n",
    "        bayes_optim_summary=machine.bayes_optim_summary,\n",
    "        estimator_class=estimator,\n",
    "        save_plots=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T20:28:02.854397Z",
     "start_time": "2019-07-20T20:15:33.532394Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(machine.bayes_optim_summary[\"estimator\"]):\n",
    "    machine.model_param_plot(\n",
    "        bayes_optim_summary=machine.bayes_optim_summary,\n",
    "        estimator_class=estimator,\n",
    "        estimator_parameter_space=estimator_parameter_space,\n",
    "        n_iter=1000,\n",
    "#         chart_scale=15,\n",
    "        title_scale=1.2,\n",
    "        save_plots=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "top_models = machine.top_bayes_optim_models(\n",
    "                bayes_optim_summary=machine.bayes_optim_summary,\n",
    "                metric=\"validation_score\",\n",
    "                num_models=1,\n",
    "            )\n",
    "top_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:14:56.712579Z",
     "start_time": "2020-03-24T16:14:56.706578Z"
    }
   },
   "outputs": [],
   "source": [
    "bayes_optim_summary[\"estimator\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:15:36.729779Z",
     "start_time": "2020-03-24T16:15:35.798633Z"
    }
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "estimator = \"LGBMRegressor\"; model_iter = 27\n",
    "# estimator = \"XGBRegressor\"; model_iter = 20\n",
    "# estimator = \"RandomForestRegressor\"; model_iter = 382\n",
    "# estimator = \"GradientBoostingRegressor\"; model_iter = 238\n",
    "# estimator = \"SVR\"; model_iter = 259\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = machine.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator_class=estimator, model_iter=model_iter\n",
    ")\n",
    "model.fit(machine.training_features.values, machine.training_target.values)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(machine.training_features, machine.training_target)\n",
    "y_pred = model.predict(machine.training_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T16:14:20.751887Z",
     "start_time": "2020-03-24T16:14:19.382675Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "machine.regression_panel(\n",
    "    model=model,\n",
    "    X_train=machine.training_features,\n",
    "    y_train=machine.training_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T15:03:38.266032Z",
     "start_time": "2020-03-24T15:03:33.916363Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "machine.regression_panel(\n",
    "    model=model,\n",
    "    X_train=machine.training_features,\n",
    "    y_train=machine.training_target,\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_valid=X_valid,\n",
    "#     y_valid=y_valid,\n",
    "#     n_folds=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = machine.regression_prediction_summary(\n",
    "    model=model,\n",
    "    X_train=machine.training_features,\n",
    "    y_train=machine.training_target,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model explanability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# estimator = \"XGBRegressor\"; model_iter = 418\n",
    "estimator = \"LGBMRegressor\"; model_iter = 27\n",
    "\n",
    "model = machine.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(machine.training_features.values, machine.training_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in machine.training_features.index[:5]:\n",
    "    machine.single_shap_viz_tree(obs_ix=i, model=model, data=machine.training_features, target=machine.training_target, classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = machine.multi_shap_viz_tree(obs_ixs=machine.training_features.index, model=model, data=machine.training_features)\n",
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = machine.multi_shap_value_tree(\n",
    "    obs_ixs=machine.training_features.index, model=model, data=machine.training_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"OverallCond\",\"LotFrontage\",\"TotalSF\",\"BsmtFinSF\",\"LotConfig\"]\n",
    "\n",
    "machine.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=machine.training_features.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "machine.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"TotalSF\",\n",
    "    color_feature=\"LotFrontage\",\n",
    "    feature_names=machine.training_features.columns.tolist(),\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = machine.training_features.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "# generate force plot\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    machine.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"OverallCond_ordinal_encoded\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=35,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "feature_names = machine.training_features.columns.tolist()\n",
    "machine.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=feature_names,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in machine.validation_features.index[:5]:\n",
    "    machine.single_shap_viz_tree(obsIx=i, model=model, data=machine.validation_features, classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = machine.multi_shap_viz_tree(obs_ixs=machine.validation_features.index, model=model, data=machine.validation_features)\n",
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = machine.multi_shap_value_tree(\n",
    "    obs_ixs=machine.validation_features.index, model=model, data=machine.validation_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"OverallCond\",\"LotFrontage\",\"TotalSF\",\"BsmtFinSF\",\"LotConfig\"]\n",
    "\n",
    "machine.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=machine.validation_features.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "machine.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"TotalSF\",\n",
    "    color_feature=\"LotFrontage\",\n",
    "    feature_names=machine.validation_features.columns.tolist(),\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = machine.validation_features.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "# generate force plot\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    machine.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=35,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "feature_names = machine.validation_features.columns.tolist()\n",
    "machine.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=feature_names,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with open(os.path.join(machine.current_experiment_dir, \"machine\", \"machine.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(machine, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"impute_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(impute_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"polynomial_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(polynomial_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"target_encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(target_encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"skew_pipe.pkl\"), 'wb') as handle:\n",
    "#     pickle.dump(skew_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"scale_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(scale_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(os.path.join(machine.current_experiment_dir, \"feature_selection\", \"FeatureSelector.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(fs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:05:17.965746Z",
     "start_time": "2019-03-24T14:05:17.669308Z"
    }
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oof_train, oof_valid, columns = machine.model_stacker(\n",
    "    models=top_models,\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    X_train=machine.training_features.values,\n",
    "    y_train=machine.training_target.values,\n",
    "    X_valid=machine.validation_features.values,\n",
    "    n_folds=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:12.912222Z",
     "start_time": "2019-03-24T14:09:44.583800Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(\n",
    "    df=pd.DataFrame(oof_train, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.776352Z",
     "start_time": "2019-03-24T14:12:03.005790Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "estimator_parameter_space = {\n",
    "    \"KernelRidge\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.000001, 15),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"polynomial\", \"rbf\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "    },\n",
    "    \"LGBMRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "    \"XGBRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    },\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000001, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00001, 10),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0001, 10),\n",
    "        \"epsilon\": hp.uniform(\"epsilon\", 0.001, 5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.983826Z",
     "start_time": "2019-03-24T14:16:24.779451Z"
    }
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "machine.exec_bayes_optim_search(\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    results_dir=\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis),\n",
    "    X=oof_train,\n",
    "    y=machine.training_target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=8,\n",
    "    n_jobs=10,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"housing\"\n",
    "rundate = \"20190807\"\n",
    "bayes_optim_summary_meta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayes_optim_summary_meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    machine.model_loss_plot(bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    machine.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary_meta,\n",
    "        estimator=estimator,\n",
    "        estimator_parameter_space=estimator_parameter_space,\n",
    "        n_iter=100,\n",
    "        chart_scale=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = machine.top_bayes_optim_models(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, num_models=1\n",
    ")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator = \"LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = machine.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "# single model evaluation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,multi model evaluation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission - stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.332889Z",
     "start_time": "2019-03-24T14:17:11.732365Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator = \"LGBMClassifier\"; model_iter = 668\n",
    "# estimator = \"XGBClassifier\"; model_iter = 380\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 411\n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 590\n",
    "# estimator = \"SVC\"; model_iter = 135\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = machine.BayesOptimModelBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(oof_train, machine.training_target.values)\n",
    "y_pred = model.predict(oof_valid)\n",
    "# print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.422440Z",
     "start_time": "2019-03-24T14:17:12.335453Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"Id\": df_test.Id, \"SalePrice\": np.expm1(y_pred)})\n",
    "submit.to_csv(\"data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
